\documentclass[DIN, pagenumber=false, fontsize=11pt, parskip=half]{scrartcl}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage[utf8]{inputenc} % this is needed for umlauts
\usepackage[T1]{fontenc} 
\usepackage{commath}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{float}
\usepackage{tikz-timing}
\usepackage{tikz}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{xstring}
\usepackage{circuitikz}
\usepackage{listings} % needed for the inclusion of source code
\usepackage[final]{pdfpages}
\usepackage{subcaption}
\usepackage{import}
\usepackage{cleveref}

\usetikzlibrary{calc,shapes.multipart,chains,arrows}

\newcommand{\Prb}[1]{P(\text{#1})}
\newcommand{\CPr}[2]{P(\text{#1}|\text{#2})}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\rank}{rank}

%Inkscape fuckery
\newcommand{\incfig}[2][\columnwidth]{%
    \def\svgwidth{#1}
    \import{./}{#2.eps_tex}
}

\title{Pattern Recognition}
\author{Tim Luchterhand, Paul Nykiel, Jonas Strauch (Group P)}

\begin{document}
    \maketitle
    \section{Linear Discriminant Analysis}
    \begin{enumerate}
        \item 
            \begin{enumerate}[label=\alph*)]
                \item
                    \begin{eqnarray*}
                        S_B &=& (\mu_1 - \mu_2) {(\mu_1 - \mu_2)}^\text{T} \\
                            &=& 
                                \begin{pmatrix}
                                    d_x \\ d_y
                                \end{pmatrix}
                                \begin{pmatrix}
                                    d_x & d_y
                                \end{pmatrix}\\
                            &=&
                                \begin{pmatrix}
                                    d_x d_x & d_x d_y \\
                                    d_x d_y & d_y d_y
                                \end{pmatrix} \\
                            &=&
                                \begin{pmatrix}
                                    5.43 & 5.59 \\
                                    5.59 & 5.76
                                \end{pmatrix}
                    \end{eqnarray*}
                \item
                    The rank of $S_B$ is 1 (assuming $\mu_1 - \mu_2 \neq \vec{0}$) as all columns are multiples of each other and thus linearly dependent.
            \end{enumerate}
        \item
            \begin{enumerate}[label=\alph*)]
                \item 
                    \textbf{Figure 3}: The variance of the points of the two classes projected on the weight vector is the largest, thus Figure 3 corresponds to the first line.
                    
                    \textbf{Figure 4:} The variance of the points of the two classes projected on the weight vector is smaller than in Figure 3 but larger than in Figure 5, thus Figure 4 corresponds to the second line.

                    \textbf{Figure 5}: The variance of the points of the two classes projected on the weight vector is the smallest, thus Figure 5 corresponds to the third line.
                \item 
                    \textbf{Figure 3}: Both $\mu_1$ and $\mu_2$ lie on the weight vector, thus $\mu_1 - \mu_2 = \tilde{\mu_1} - \tilde{\mu_2}$, using the values given in Equation 4 the distance can be calculated:
                    \begin{equation*}
                        {(\tilde{\mu_1} - \tilde{\mu_2})}^2
                            = {(\mu_1 - \mu_2)}^2
                            = d_x^2 + d_y^2
                            = 11.19
                    \end{equation*}

                    \textbf{Figure 4}: Both $\mu_1$ and $\mu_2$ are projected onto the same point ${(0,0)}^\text{T}$, thus $\tilde{\mu_1} = \tilde{\mu_2}$, this yields ${(\tilde{\mu_1} - \tilde{\mu_2})}^2 = 0$.

                    \textbf{Figure 5}: First the two mean points need to be transformed:
                    \begin{eqnarray*}
                        \tilde{\mu_1} &=& w^\text{T} \mu_1 \\
                        &=& 
                            \begin{pmatrix}
                                0.25 & -0.97
                            \end{pmatrix}
                            \begin{pmatrix}
                                -1.17 \\ -1.20
                            \end{pmatrix} \\
                        &=& -1.46 \\
                        \tilde{\mu_2} &=& w^\text{T} \mu_2 \\
                        &=& 
                            \begin{pmatrix}
                                0.25 & -0.97
                            \end{pmatrix}
                            \begin{pmatrix}
                                1.17 \\ 1.20
                            \end{pmatrix} \\
                        &=& 1.46
                    \end{eqnarray*}
                    Next the (quadratic) distance can be calculated:
                    \begin{equation*}
                        {(\tilde{\mu_1} - \tilde{\mu_2})}^2 
                            = {(-1.46 - 1.46)}^2
                            = 8.53
                    \end{equation*}
                \item
                    \textbf{Figure 3}: The vector $w_1$ connects $\mu_1$ and $\mu_2$, thus the direction (i.e. not normalized) is given by $\mu_2 - \mu_1$.
                    The actual vector is this difference vector normalized:
                    \begin{equation*}
                        w_1 = \frac{\mu_2 - \mu_1}{\norm{\mu_2 - \mu_1}}
                            = 
                            \begin{pmatrix}
                                0.70 \\ 0.72
                            \end{pmatrix}
                    \end{equation*}

                    \textbf{Figure 4}: The vector $w_2$ is orthogonal to the connection line and thus $w_1$, this vector can be calculated by rotating 
                    $w_1$ by $-\frac{\pi}{2}$:
                    \begin{equation*}
                        w_2 = 
                            \begin{pmatrix}
                                0 & 1\\
                                -1 & 0
                            \end{pmatrix}
                            w_1 
                            =
                            \begin{pmatrix}
                                0.72 \\ -0.70
                            \end{pmatrix}
                    \end{equation*}
                \item
                    First line:
                    \begin{equation*}
                        J(w) = \frac{{(\tilde{\mu_1} - \tilde{\mu_2})}^2}{\tilde{\sigma}_1^2 + \tilde{\sigma}_2^2}
                            = \frac{11.19}{1.40 + 2.09}
                            = 3.21
                    \end{equation*}
                    Second line:
                    \begin{equation*}
                        J(w) = \frac{{(\tilde{\mu_1} - \tilde{\mu_2})}^2}{\tilde{\sigma}_1^2 + \tilde{\sigma}_2^2}
                            = \frac{0}{0.28 + 0.74}
                            = 0
                    \end{equation*}
                    Third line:
                    \begin{equation*}
                        J(w) = \frac{{(\tilde{\mu_1} - \tilde{\mu_2})}^2}{\tilde{\sigma}_1^2 + \tilde{\sigma}_2^2}
                            = \frac{8.53}{0.16 + 0.06}
                            = 38.77
                    \end{equation*}
            \end{enumerate}
        \item
            By maximizing the distance between the means of the classes the projected standard deviation, and thus the variance, of the classes is not taken into account. As a result the projected size of the classes is not considered and the classes can overlap in the projected space.
        \item 
            \begin{enumerate}[label=\alph*)]
                \item
                    The rank-inequality of sylvester is:
                    \begin{equation*}
                        \rank(A B) \leq \min \{ \rank(A), \rank(B) \}
                    \end{equation*}
                    Thus it holds, that
                    \begin{equation*}
                        \rank(S_W^{-1} S_B) \leq \min \{\rank(S_W^{-1}), \rank(S_B)\} 
                            = \min \{\rank(S_W^{-1}), 1\} \leq 1
                    \end{equation*}
                    Hence there is at maximum one linear independent vector in $S_W^{-1} S_B$,
                    as a result the variance along the second axis of the matrix is zero,
                    thus the second eigenvalue is 0.
                \item
                    \begin{eqnarray*}
                        \tilde{x}_1 &=& u_1^\text{T} x_1 \\
                            &=& \begin{pmatrix} 0.25 & -0.97 \end{pmatrix}
                                \begin{pmatrix} 0.38 \\ -0.42 \end{pmatrix} \\
                            &=& 0.50 \\
                        \tilde{x}_2 &=& u_1^\text{T} x_2 \\
                            &=& \begin{pmatrix} 0.25 & -0.97 \end{pmatrix}
                                \begin{pmatrix} 1.88 \\ 1.08 \end{pmatrix} \\
                            &=& -0.58
                    \end{eqnarray*}
                \item The first component of the vector is weighted by $0.25$, the second
                    component by $0.97$, thus the second axis, that is $y$, is more important.
                \item For the given data points every threshold $-0.58 < w_0 < 0.5$
                    is able to divide the data, the maximal separation is achieved
                    for a $w_0$ in the middle of both points:
                    \begin{equation*}
                        w_0 = \frac{\tilde{x}_1 + \tilde{x}_2}{2} = -0.04
                    \end{equation*}
            \end{enumerate}
    \end{enumerate}
\end{document}
